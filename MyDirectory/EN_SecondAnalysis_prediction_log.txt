###########################
######### context #########
###########################
The scikit-learn (sklearn)-based Python workflow independently supports both modeling (i.e., training and testing) and prediction (i.e., using a pre-built model), and implements 4 feature selection methods, 19 model regressors, hyperparameter tuning, performance metric computation, feature and permutation importance analyses, prediction interval estimation, execution monitoring via progress bars, and parallel processing.
###########################
######## reference ########
###########################
An article might potentially be published in the future.
###########################
###### repositories  ######
###########################
Please cite:
 GitHub (https://github.com/Nicolas-Radomski/GenomicBasedRegression),
 Docker Hub (https://hub.docker.com/r/nicolasradomski/genomicbasedregression),
 and/or Anaconda Hub (https://anaconda.org/nicolasradomski/genomicbasedregression).
###########################
#### acknowledgements  ####
###########################
Many thanks to Andrea De Ruvo, Adriano Di Pasquale and ChatGPT for the insightful discussions that helped improve the algorithm.
###########################
######## versions  ########
###########################
GenomicBasedRegression: 1.2.0 (released in November 2025)
python: 3.12
argparse: 1.1
catboost: 1.2.8
joblib: 1.5.1
lightgbm: 4.6.0
numpy: 1.26.4
pandas: 2.2.2
pickle: 4.0
re: 2.2.1
scipy: 1.16.0
sklearn: 1.5.2
tqdm: 4.67.1
tqdm-joblib: 0.0.4
xgboost: 2.1.3
###########################
######## arguments  #######
###########################
subcommand: prediction
inputpath_mutations: genomic_profils_for_prediction.tsv
inputpath_features: MyDirectory/EN_FirstAnalysis_features.obj
inputpath_feature_encoder: MyDirectory/EN_FirstAnalysis_feature_encoder.obj
inputpath_calibration_features: MyDirectory/EN_FirstAnalysis_calibration_features.obj
inputpath_calibration_targets: MyDirectory/EN_FirstAnalysis_calibration_targets.obj
inputpath_model: MyDirectory/EN_FirstAnalysis_model.obj
alpha: 0.05
outputpath: MyDirectory
prefix: EN_SecondAnalysis
digits: 6
debug: 0
warnings: False
nocheck: False
###########################
######### checks  #########
###########################
The traceback level was set to 0
The warnings were ignored
The recommended versions of Python and packages were properly controlled
The prediction subcommand was used
The minimum required number of samples in the dataset (i.e., >= 1) and the expected number of columns (i.e., >= 3) in the input file of mutations were properly controlled (i.e., 20 and 12, respectively)
The input tested mutations include all features required by the trained one-hot encoder
The following unexpected features in the input tested mutations will be ignored for one-hot encoding: ['Locus_11']
The encoded features between training and prediction datasets were confirmed as identical
The 10 provided features were one-hot encoded into 78 encoded features
The pipeline expected 50 one-hot encoded features to perform prediction
The pipeline components of the provided best model were properly recognized: Pipeline(steps=[('feature_selection', SelectFromModel(estimator=RandomForestRegressor(max_depth=10, n_jobs=1, random_state=42), max_features=50, threshold=-inf)), ('model', ElasticNet(alpha=0.01, max_iter=5000, random_state=42, selection='random'))])
The one-hot encoded prediction matrix was reindexed and aligned to match the exact feature names and order expected by the trained pipeline
The prediction intervals (i.e., 95.0%) were calculated using a significance level of alpha = 0.05
The output directory already existed
###########################
####### execution  ########
###########################
The script started on 2025-11-19 10:18:58.358291
The script stoped on 2025-11-19 10:18:58.436843
The script lasted 0 days, 0 hrs, 0 mins and 0.08 secs (i.e., 0.08 secs in total)
###########################
###### output  files ######
###########################
MyDirectory/EN_SecondAnalysis_prediction.tsv
MyDirectory/EN_SecondAnalysis_prediction_log.txt
###########################
### prediction  dataset ###
###########################
 sample  prediction     lower     upper
S2.1.01   25.611378 17.995161 33.227595
S2.1.02   24.969018 17.352801 32.585235
S2.1.03   17.271247  9.655030 24.887464
S2.1.04   40.675286 33.059069 48.291503
S2.1.05   16.880948  9.264731 24.497165
S2.1.06   17.271247  9.655030 24.887464
S2.1.07   32.190493 24.574276 39.806710
S2.1.08   17.271247  9.655030 24.887464
S2.1.09   54.831289 47.215072 62.447506
S2.1.10   46.860149 39.243932 54.476366
S2.1.11   21.458852 13.842635 29.075069
S2.1.12   50.893483 43.277266 58.509700
S2.1.13   21.003959 13.387742 28.620176
S2.1.14   16.273397  8.657180 23.889614
S2.1.15   12.404974  4.788757 20.021191
S2.1.16   17.090679  9.474462 24.706896
S2.1.17   17.939645 10.323428 25.555862
S2.1.18    6.328265 -1.287952 13.944482
S2.1.19    2.663949 -4.952268 10.280166
S2.1.20    3.516674 -4.099543 11.132891
Note: Up to 20 results are displayed in the log for monitoring purposes, while the full set of results is available in the output files. 
Lower and upper correspond to the range of the prediction intervals. 
