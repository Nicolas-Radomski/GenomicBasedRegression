###########################
######### context #########
###########################
The scikit-learn (sklearn)-based Python workflow independently supports both modeling (i.e., training and testing) and prediction (i.e., using a pre-built model), and implements 5 feature selection methods, 19 model regressors, hyperparameter tuning, performance metric computation, feature and permutation importance analyses, prediction interval estimation, execution monitoring via progress bars, and parallel processing.
###########################
######## reference ########
###########################
An article might potentially be published in the future.
###########################
###### repositories  ######
###########################
Please cite:
 GitHub (https://github.com/Nicolas-Radomski/GenomicBasedRegression),
 Docker Hub (https://hub.docker.com/r/nicolasradomski/genomicbasedregression),
 and/or Anaconda Hub (https://anaconda.org/nicolasradomski/genomicbasedregression).
###########################
#### acknowledgements  ####
###########################
Many thanks to Andrea De Ruvo, Adriano Di Pasquale and ChatGPT for the insightful discussions that helped improve the algorithm.
###########################
######## versions  ########
###########################
GenomicBasedRegression: 1.3.0 (released in December 2025)
python: 3.12
argparse: 1.1
scipy: 1.16.0
pandas: 2.2.2
sklearn: 1.5.2
pickle: 4.0
catboost: 1.2.8
lightgbm: 4.6.0
xgboost: 2.1.3
numpy: 1.26.4
joblib: 1.5.1
tqdm: 4.67.1
tqdm-joblib: 0.0.4
###########################
######## arguments  #######
###########################
subcommand: prediction
inputpath_mutations: genomic_profils_for_prediction.tsv
inputpath_features: MyDirectory/EN_FirstAnalysis_features.obj
inputpath_feature_encoder: MyDirectory/EN_FirstAnalysis_feature_encoder.obj
inputpath_calibration_features: MyDirectory/EN_FirstAnalysis_calibration_features.obj
inputpath_calibration_targets: MyDirectory/EN_FirstAnalysis_calibration_targets.obj
inputpath_model: MyDirectory/EN_FirstAnalysis_model.obj
alpha: 0.05
outputpath: MyDirectory
prefix: EN_SecondAnalysis
digits: 6
debug: 0
warnings: False
nocheck: False
###########################
######### checks  #########
###########################
The traceback level was set to 0
The warnings were ignored
The recommended versions of Python and packages were properly controlled
The prediction subcommand was used
The minimum required number of samples in the dataset (i.e., >= 1) and the expected number of columns (i.e., >= 3) in the input file of mutations were properly controlled (i.e., 20 and 12, respectively)
The input tested mutations include all features required by the trained one-hot encoder
The following unexpected features in the input tested mutations will be ignored for one-hot encoding: ['Locus_11']
The encoded features between training and prediction datasets were confirmed as identical
The 10 provided features were one-hot encoded into 80 encoded features
The pipeline expected 50 one-hot encoded features to perform prediction
The pipeline components of the provided best model were properly recognized: Pipeline(steps=[('feature_selection', SelectFromModel(estimator=Ridge(max_iter=1000, random_state=42, tol=0.001), max_features=50, threshold=-inf)), ('model', ElasticNet(alpha=0.01, l1_ratio=0.8, max_iter=5000, random_state=42, selection='random'))])
The one-hot encoded prediction matrix was reindexed and aligned to match the exact feature names and order expected by the trained pipeline
The prediction intervals (i.e., 95.0%) were calculated using a significance level of alpha = 0.05
The output directory already existed
###########################
####### execution  ########
###########################
The script started on 2025-12-16 16:13:22.162498
The script stoped on 2025-12-16 16:13:22.192239
The script lasted 0 days, 0 hrs, 0 mins and 0.03 secs (i.e., 0.03 secs in total)
###########################
###### output  files ######
###########################
MyDirectory/EN_SecondAnalysis_prediction.tsv
MyDirectory/EN_SecondAnalysis_prediction_log.txt
###########################
### prediction  dataset ###
###########################
 sample  prediction     lower     upper
S2.1.01   24.576393 17.121827 32.030958
S2.1.02   25.041019 17.586454 32.495585
S2.1.03   16.918525  9.463960 24.373091
S2.1.04   40.945235 33.490669 48.399801
S2.1.05   15.487894  8.033329 22.942460
S2.1.06   15.835462  8.380897 23.290028
S2.1.07   33.569069 26.114503 41.023634
S2.1.08   17.183770  9.729204 24.638335
S2.1.09   50.240448 42.785882 57.695013
S2.1.10   46.088282 38.633716 53.542847
S2.1.11   19.478664 12.024099 26.933230
S2.1.12   50.705692 43.251126 58.160257
S2.1.13   18.889800 11.435234 26.344365
S2.1.14   14.041691  6.587125 21.496256
S2.1.15    9.891962  2.437396 17.346527
S2.1.16   15.441937  7.987371 22.896502
S2.1.17   16.285103  8.830537 23.739668
S2.1.18    6.660708 -0.793858 14.115273
S2.1.19    2.695833 -4.758732 10.150399
S2.1.20    3.308829 -4.145736 10.763395
Note: Up to 20 results are displayed in the log for monitoring purposes, while the full set of results is available in the output files. 
Lower and upper correspond to the range of the prediction intervals. 
