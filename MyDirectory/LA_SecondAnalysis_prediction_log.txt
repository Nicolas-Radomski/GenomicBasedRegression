###########################
######### context #########
###########################
The scikit-learn (sklearn)-based Python workflow independently supports both modeling (i.e., training and testing) and prediction (i.e., using a pre-built model), and implements 5 feature selection methods, 19 model regressors, hyperparameter tuning, performance metric computation, feature and permutation importance analyses, prediction interval estimation, execution monitoring via progress bars, and parallel processing.
###########################
######## reference ########
###########################
An article might potentially be published in the future.
###########################
###### repositories  ######
###########################
Please cite:
 GitHub (https://github.com/Nicolas-Radomski/GenomicBasedRegression),
 Docker Hub (https://hub.docker.com/r/nicolasradomski/genomicbasedregression),
 and/or Anaconda Hub (https://anaconda.org/nicolasradomski/genomicbasedregression).
###########################
#### acknowledgements  ####
###########################
Many thanks to Andrea De Ruvo, Adriano Di Pasquale and ChatGPT for the insightful discussions that helped improve the algorithm.
###########################
######## versions  ########
###########################
GenomicBasedRegression: 1.3.0 (released in December 2025)
python: 3.12
argparse: 1.1
scipy: 1.16.0
pandas: 2.2.2
sklearn: 1.5.2
pickle: 4.0
catboost: 1.2.8
lightgbm: 4.6.0
xgboost: 2.1.3
numpy: 1.26.4
joblib: 1.5.1
tqdm: 4.67.1
tqdm-joblib: 0.0.4
###########################
######## arguments  #######
###########################
subcommand: prediction
inputpath_mutations: genomic_profils_for_prediction.tsv
inputpath_features: MyDirectory/LA_FirstAnalysis_features.obj
inputpath_feature_encoder: MyDirectory/LA_FirstAnalysis_feature_encoder.obj
inputpath_calibration_features: MyDirectory/LA_FirstAnalysis_calibration_features.obj
inputpath_calibration_targets: MyDirectory/LA_FirstAnalysis_calibration_targets.obj
inputpath_model: MyDirectory/LA_FirstAnalysis_model.obj
alpha: 0.05
outputpath: MyDirectory
prefix: LA_SecondAnalysis
digits: 6
debug: 0
warnings: False
nocheck: False
###########################
######### checks  #########
###########################
The traceback level was set to 0
The warnings were ignored
The recommended versions of Python and packages were properly controlled
The prediction subcommand was used
The minimum required number of samples in the dataset (i.e., >= 1) and the expected number of columns (i.e., >= 3) in the input file of mutations were properly controlled (i.e., 20 and 12, respectively)
The input tested mutations include all features required by the trained one-hot encoder
The following unexpected features in the input tested mutations will be ignored for one-hot encoding: ['Locus_11']
The encoded features between training and prediction datasets were confirmed as identical
The 10 provided features were one-hot encoded into 80 encoded features
The pipeline expected 50 one-hot encoded features to perform prediction
The pipeline components of the provided best model were properly recognized: Pipeline(steps=[('feature_selection', SelectKBest(k=50, score_func=<function mutual_info_regression at 0x7fd34f704540>)), ('model', Lasso(alpha=0.0001, max_iter=5000, random_state=42, tol=1e-05))])
The one-hot encoded prediction matrix was reindexed and aligned to match the exact feature names and order expected by the trained pipeline
The prediction intervals (i.e., 95.0%) were calculated using a significance level of alpha = 0.05
The output directory already existed
###########################
####### execution  ########
###########################
The script started on 2025-12-16 16:15:52.554432
The script stoped on 2025-12-16 16:15:52.586801
The script lasted 0 days, 0 hrs, 0 mins and 0.03 secs (i.e., 0.03 secs in total)
###########################
###### output  files ######
###########################
MyDirectory/LA_SecondAnalysis_prediction.tsv
MyDirectory/LA_SecondAnalysis_prediction_log.txt
###########################
### prediction  dataset ###
###########################
 sample  prediction     lower     upper
S2.1.01   28.420972 21.215057 35.626888
S2.1.02   28.871869 21.665953 36.077784
S2.1.03   20.432514 13.226599 27.638430
S2.1.04   24.540833 17.334918 31.746749
S2.1.05   19.139259 11.933343 26.345174
S2.1.06   20.094522 12.888606 27.300437
S2.1.07   45.584592 38.378677 52.790508
S2.1.08   20.800705 13.594789 28.006620
S2.1.09   55.086511 47.880595 62.292427
S2.1.10   46.001199 38.795283 53.207115
S2.1.11   17.935265 10.729349 25.141180
S2.1.12   47.999486 40.793570 55.205401
S2.1.13   18.588243 11.382328 25.794159
S2.1.14   30.970468 23.764553 38.176384
S2.1.15   27.208391 20.002475 34.414307
S2.1.16   32.047841 24.841925 39.253757
S2.1.17   33.578379 26.372463 40.784294
S2.1.18    5.717077 -1.488839 12.922993
S2.1.19    2.620578 -4.585338  9.826493
S2.1.20    3.306440 -3.899475 10.512356
Note: Up to 20 results are displayed in the log for monitoring purposes, while the full set of results is available in the output files. 
Lower and upper correspond to the range of the prediction intervals. 
