###########################
######### context #########
###########################
The scikit-learn (sklearn)-based Python workflow independently supports both modeling (i.e., training and testing) and prediction (i.e., using a pre-built model), and implements 4 feature selection methods, 19 model regressors, hyperparameter tuning, performance metric computation, feature and permutation importance analyses, prediction interval estimation, execution monitoring via progress bars, and parallel processing.
###########################
######## reference ########
###########################
An article might potentially be published in the future.
###########################
###### repositories  ######
###########################
Please cite:
 GitHub (https://github.com/Nicolas-Radomski/GenomicBasedRegression),
 Docker Hub (https://hub.docker.com/r/nicolasradomski/genomicbasedregression),
 and/or Anaconda Hub (https://anaconda.org/nicolasradomski/genomicbasedregression).
###########################
#### acknowledgements  ####
###########################
Many thanks to Andrea De Ruvo, Adriano Di Pasquale and ChatGPT for the insightful discussions that helped improve the algorithm.
###########################
######## versions  ########
###########################
GenomicBasedRegression: 1.2.0 (released in November 2025)
python: 3.12
argparse: 1.1
catboost: 1.2.8
joblib: 1.5.1
lightgbm: 4.6.0
numpy: 1.26.4
pandas: 2.2.2
pickle: 4.0
re: 2.2.1
scipy: 1.16.0
sklearn: 1.5.2
tqdm: 4.67.1
tqdm-joblib: 0.0.4
xgboost: 2.1.3
###########################
######## arguments  #######
###########################
subcommand: prediction
inputpath_mutations: genomic_profils_for_prediction.tsv
inputpath_features: MyDirectory/LA_FirstAnalysis_features.obj
inputpath_feature_encoder: MyDirectory/LA_FirstAnalysis_feature_encoder.obj
inputpath_calibration_features: MyDirectory/LA_FirstAnalysis_calibration_features.obj
inputpath_calibration_targets: MyDirectory/LA_FirstAnalysis_calibration_targets.obj
inputpath_model: MyDirectory/LA_FirstAnalysis_model.obj
alpha: 0.05
outputpath: MyDirectory
prefix: LA_SecondAnalysis
digits: 6
debug: 0
warnings: False
nocheck: False
###########################
######### checks  #########
###########################
The traceback level was set to 0
The warnings were ignored
The recommended versions of Python and packages were properly controlled
The prediction subcommand was used
The minimum required number of samples in the dataset (i.e., >= 1) and the expected number of columns (i.e., >= 3) in the input file of mutations were properly controlled (i.e., 20 and 12, respectively)
The input tested mutations include all features required by the trained one-hot encoder
The following unexpected features in the input tested mutations will be ignored for one-hot encoding: ['Locus_11']
The encoded features between training and prediction datasets were confirmed as identical
The 10 provided features were one-hot encoded into 78 encoded features
The pipeline expected 50 one-hot encoded features to perform prediction
The pipeline components of the provided best model were properly recognized: Pipeline(steps=[('feature_selection', SelectKBest(k=50, score_func=<function mutual_info_regression at 0x7fb341070540>)), ('model', Lasso(alpha=0.0001, max_iter=10000, random_state=42, tol=1e-05))])
The one-hot encoded prediction matrix was reindexed and aligned to match the exact feature names and order expected by the trained pipeline
The prediction intervals (i.e., 95.0%) were calculated using a significance level of alpha = 0.05
The output directory already existed
###########################
####### execution  ########
###########################
The script started on 2025-11-19 10:21:43.962291
The script stoped on 2025-11-19 10:21:43.985028
The script lasted 0 days, 0 hrs, 0 mins and 0.02 secs (i.e., 0.02 secs in total)
###########################
###### output  files ######
###########################
MyDirectory/LA_SecondAnalysis_prediction.tsv
MyDirectory/LA_SecondAnalysis_prediction_log.txt
###########################
### prediction  dataset ###
###########################
 sample  prediction     lower     upper
S2.1.01   32.257911 24.840270 39.675552
S2.1.02   31.572700 24.155059 38.990341
S2.1.03   17.117038  9.699397 24.534679
S2.1.04   22.911521 15.493880 30.329162
S2.1.05   15.244046  7.826405 22.661687
S2.1.06   16.667578  9.249937 24.085219
S2.1.07   32.968649 25.551008 40.386290
S2.1.08   16.613165  9.195524 24.030806
S2.1.09   61.318817 53.901177 68.736458
S2.1.10   46.002436 38.584795 53.420077
S2.1.11   14.543389  7.125748 21.961030
S2.1.12   48.000014 40.582373 55.417655
S2.1.13   20.840789 13.423148 28.258430
S2.1.14   30.157660 22.740019 37.575301
S2.1.15   27.430537 20.012896 34.848178
S2.1.16   29.423176 22.005535 36.840817
S2.1.17   32.175680 24.758039 39.593321
S2.1.18    6.094088 -1.323553 13.511729
S2.1.19    2.571917 -4.845724  9.989558
S2.1.20    3.611730 -3.805911 11.029371
Note: Up to 20 results are displayed in the log for monitoring purposes, while the full set of results is available in the output files. 
Lower and upper correspond to the range of the prediction intervals. 
